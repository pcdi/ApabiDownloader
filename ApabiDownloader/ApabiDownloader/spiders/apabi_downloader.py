from datetime import datetime, timezone
from pathlib import Path
from urllib import parse

import scrapy


def authentication_failed(response: scrapy.http.TextResponse) -> bool:
    """Checks if user authentication has failed.

    Args:
        response: A scrapy.http.TextResponse instance generated by the after_login()
          function.

    Returns:
        bool: True if the user is not logged in, False otherwise

    """
    # If the response contains "您尚未登陆。", then the user is not yet logged in,
    # which means that the authentication has failed.
    if auth_failed := response.xpath("//p/text()").re("您尚未登陆。"):
        return auth_failed


class ApabiDownloaderSpider(scrapy.Spider):
    name = "apabi_downloader"
    output_dir_base = "output/"

    def __init__(self, book_detail_url: str = None, *args, **kwargs) -> None:
        """Initializes the spider instance with the book details.

        Args:
            book_detail_url: The URL of the book detail page in the Apabi digital
              library. This URL must contain "book.detail" and point to the page that
              contains the "在线阅读" (read online) link.
            *args:
            **kwargs:
        """
        super(ApabiDownloaderSpider, self).__init__(*args, **kwargs)
        self.allowed_domains = [parse.urlparse(book_detail_url).netloc]
        self.var_dict = None
        self.page_total = None
        self.downloaded_items = None
        self.book_detail_url = book_detail_url
        self.online_reader_url = None
        self.img_url = None
        self.output_folder_name = None
        self.output_dir = None
        self.make_output_dir()

    def make_output_dir(self) -> None:
        """Creates the output folder(s) to save the scraped page images.

        The root output directory is set via the class. Every instance creates the
        root output directory if not already present. Inside the root output
        directory, each instance creates a directory with the book's "metaid" as its
        name. Inside this folder, it saves the book's scraped page images.
        """
        try:
            self.output_folder_name = parse.parse_qs(self.book_detail_url)["metaid"][
                0
            ].lstrip("m.")
            self.output_dir = self.output_dir_base + self.output_folder_name
            self.logger.info(
                f"Making output directory. Saving to " f"{self.output_folder_name}."
            )
            Path(self.output_dir_base).mkdir(exist_ok=True)
            Path(self.output_dir).mkdir(exist_ok=True)
        except Exception as e:
            self.logger.exception(e)
            raise

    def start_requests(self):
        # First, we need to log in via IP. Then we need to access the book details
        # page where we obtain the link to an "OnLineReader" page, which serves the
        # book's page images. From there, we can download these images.
        start_url = parse.urljoin(self.book_detail_url, "pub.mvc/?pid=login&cult=CN")
        yield from [scrapy.Request(url=start_url, callback=self.parse)]

    def parse(self, response):
        yield scrapy.FormRequest.from_response(
            response=response,
            formdata={"LoginType": "IPAutoLogin", "cult": "CN"},
            callback=self.after_login,
        )

    def after_login(self, response):
        if authentication_failed(response):
            self.logger.error("Login failed.")
            return
        self.logger.info("Login successful.")
        yield scrapy.Request(
            url=self.book_detail_url,
            callback=self.after_book_infopage,
            dont_filter=True,
        )

    def after_book_infopage(self, response):
        self.logger.info("Got book info page.")
        if online_read_path := response.xpath(
            '//a[contains(@type, "onlineread")]'
        ).attrib["href"]:
            self.online_reader_url = response.urljoin(online_read_path)
            yield from self.request_online_reader_page()

    def request_online_reader_page(self):
        yield scrapy.Request(
            self.online_reader_url,
            callback=self.on_online_read_page,
            dont_filter=True,
        )

    def create_var_dict(self, response):
        # The variables that are needed to construct valid requests are saved in
        # hidden <input> tags in the HTML, which are in turn read by the JavaScript
        # via their ids.
        var_dict = {}
        for var in response.xpath("//input"):
            if "value" in var.attrib:
                var_dict.update({var.attrib["id"]: var.attrib["value"]})
        # Some values are hidden as a string in one entry, need to dissect that
        var_dict.update(parse.parse_qsl(var_dict["urlrights"]))
        self.var_dict = var_dict
        self.logger.info(
            f'Downloading {self.var_dict["bookName"]} by {self.var_dict["creator"]}.'
        )
        self.logger.info(f'Access is provided by {self.var_dict["txtOrgIdentifier"]}.')
        self.logger.info(
            "Next timeout on {}.".format(
                datetime.strptime(self.var_dict["time"], "%Y-%m-%d %H:%M:%S")
                .replace(tzinfo=timezone.utc)
                .astimezone()
                .isoformat()
            )
        )

    def on_online_read_page(self, response):
        self.logger.info("Got online read page.")
        self.create_var_dict(response)
        if self.page_total is None:
            yield from self.get_page_total(response)
        else:
            yield from self.get_img_url(response)

    def get_page_total(self, response):
        self.logger.info("Getting page total.")
        # The total page number of the book is not readily available but needs to be
        # requested through "Command/Getcontent.ashx", which returns an XML file with
        # the table of contents/book structure as well as the page total.
        # See reader.js:1692.
        yield scrapy.FormRequest(
            url=response.urljoin("Command/Getcontent.ashx"),
            method="GET",
            formdata={
                "OrgIdentifier": self.var_dict["txtOrgIdentifier"],
                "objID": self.var_dict["txtFileID"],
                "parentIndex": "0",
                "ServiceType": "getcontent",
                "metaId": self.var_dict["txtMetaId"],
                "OrgId": self.var_dict["txtOrgIdentifier"],
                "SessionId": self.var_dict["txtSessionId"],
                "cult": self.var_dict["txtCultureName"],
                "UserName": self.var_dict["txtuserName"],
            },
            callback=self.get_img_url,
            dont_filter=True,
        )

    def set_page_total(self, response):
        self.page_total = int(response.xpath("//Content").attrib["TotalNum"])
        self.logger.info(f"Book has {self.page_total} pages.")

    def get_img_url(self, response):
        if self.page_total is None:
            self.set_page_total(response)
        self.downloaded_items = []
        for page in range(self.page_total):
            if Path(f"{self.output_dir}/{str(page + 1)}.png").is_file():
                self.downloaded_items.append(page + 1)
        self.img_url = response.urljoin("command/imagepage.ashx")
        page = self.get_next_page_to_download(start_page=1)
        if page <= self.page_total:
            yield from self.request_img(page)

    def get_next_page_to_download(self, start_page):
        page = start_page
        while page in self.downloaded_items:
            page += 1
        return page

    def is_timed_out(self):
        # Each session has a fixed timeout time after which it is not possible to
        # access any image pages anymore. This timeout is hardcoded to 20 Minutes,
        # which is saved in variable "txtOnlineViewTime". The actual timeout time is
        # calculated when the "OnLineReader" page is first accessed. It is saved in
        # the variable "txtReadSignTime" and also in "time" inside "urlrights".
        timeout_time = datetime.strptime(self.var_dict["time"], "%Y-%m-%d %H:%M:%S")
        time_left = timeout_time - datetime.utcnow()
        if time_left.seconds > 30:
            return False
        else:
            return True

    def get_new_timeslot(self):
        self.logger.info("Timeout, getting new timeslot.")
        yield from self.request_online_reader_page()

    def request_img(self, page):
        if self.is_timed_out():
            yield from self.get_new_timeslot()
        else:
            self.logger.info(f"Getting page {page} of {self.page_total}.")
            # Requests to get the images are GET requests to "command/imagepage.ashx"
            # that are called by the function getUrl(page) from reader.js:657. The
            # parameters are initialized by window.onload, see also reader.js:2499.
            # "command/imagepage.ashx" returns a PNG as a string.
            yield scrapy.FormRequest(
                url=self.img_url,
                method="GET",
                formdata={
                    "objID": self.var_dict["txtFileID"],
                    "metaId": self.var_dict["txtMetaId"],
                    "OrgId": self.var_dict["txtOrgIdentifier"],
                    "scale": "1",
                    "width": "9999",
                    "height": "9999",
                    "pageid": str(page),
                    "ServiceType": "Imagepage",
                    "SessionId": self.var_dict["txtSessionId"],
                    "UserName": self.var_dict["txtuserName"],
                    "cult": self.var_dict["txtCultureName"],
                    "rights": self.var_dict["rights"],
                    "time": self.var_dict["time"],
                    "sign": self.var_dict["sign"],
                },
                callback=self.get_image,
                cb_kwargs={"page": page},
                meta={"handle_httpstatus_list": [403]},
            )

    def get_image(self, response, page):
        # If the session is timed out, GET requests to "command/imagepage.ashx" will
        # return HTTP status 403.
        if response.status == 403 or self.is_timed_out() is True:
            yield from self.get_new_timeslot()
        else:
            self.logger.info(f"Got page {page} of {self.page_total}.")
            yield {"page": str(page), "image": response.body}
            next_page = self.get_next_page_to_download(page + 1)
            if next_page <= self.page_total:
                yield from self.request_img(next_page)
